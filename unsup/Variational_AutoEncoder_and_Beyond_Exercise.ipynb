{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Variational AutoEncoder and Beyond Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eemlcommunity/PracticalSessions2020/blob/master/unsup/Variational_AutoEncoder_and_Beyond_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McyykqPe_ht2",
        "colab_type": "text"
      },
      "source": [
        "# Variational AutoEncoder and Beyond\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRJoJwz7_u-i",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Unsupervised learning and generative models are key building blocks of machine learning systems. This tutorial covers a specific class of unsupervised models called autoencoders. Autoencoders have been a popular model of choice for learning representations of data as well as modeling the data distribution.\n",
        "\n",
        "We implement and train three variants:\n",
        "* the original vanilla autoencoder,\n",
        "* variational autoencoder, and\n",
        "* as homework the recently introduced regularized autoencoders.\n",
        "\n",
        "Along with training these, we examine and investigate their generative modeling capabilities.\n",
        "\n",
        "## Organization\n",
        "\n",
        "We start by loading all libraries and inspecting the data.\n",
        "\n",
        "In the first part of the tutorial we will train an AutoEncoder. This will serve as a foundation for implementing and understanding Variational AutoEncoders. The homework will be to implement Regularized AutoEncoder, a recent model that generates good samples but foregoes the need to use variational inference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmxCc8SHmoPH",
        "colab_type": "text"
      },
      "source": [
        "# Task 1: Install haiku and load data\n",
        "\n",
        "This code is all written for you. You only need to run it and inspect the returned data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSLcrpt2mmy1",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title RUN this cell to install and import required libraries\n",
        "!pip install -q dm-haiku\n",
        "\n",
        "from typing import Any, Generator, Mapping, Tuple, NamedTuple, Sequence\n",
        "\n",
        "from collections import defaultdict, namedtuple\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import haiku as hk\n",
        "import jax\n",
        "from jax.experimental import optix\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow.image\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import scipy.stats\n",
        "\n",
        "import functools\n",
        "\n",
        "%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EGvpG0Zzj8a",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Python Type Declarations\n",
        "OptState = Any\n",
        "PRNGKey = jnp.ndarray\n",
        "Batch = Mapping[str, np.ndarray]\n",
        "MNIST_IMAGE_SHAPE: Sequence[int] = (28, 28, 1)\n",
        "\n",
        "CLASS_LABELS = [\n",
        "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
        "    'Shirt', 'Sneaker', 'Bag', 'Ankle boot',\n",
        "    # 'zero', 'one', 'two', 'three', 'four', 'five', 'six',\n",
        "    # 'seven', 'eight', 'nine'\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HahBKi63AvHI",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Utility Functions for Data\n",
        "\n",
        "def gallery(\n",
        "    images, labels: Sequence[int] = None,\n",
        "    max_images: int = 10, max_fig_size=(30, 30)):\n",
        "  \n",
        "  num_frames, h, w, num_channels = images.shape\n",
        "  num_frames = min(num_frames, max_images)\n",
        "  ff, axes = plt.subplots(1, num_frames,\n",
        "                          figsize=max_fig_size,\n",
        "                          subplot_kw={'xticks': [], 'yticks': []})\n",
        "  if num_frames == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "  # All outputs in the notebook are supposed to be between 0 and 1\n",
        "  images = np.clip(images, 0.0, 1.0)\n",
        "  \n",
        "  for i in range(0, num_frames):\n",
        "    if num_channels == 3:\n",
        "      axes[i].imshow(np.squeeze(images[i]))\n",
        "    else:\n",
        "      axes[i].imshow(np.squeeze(images[i]), cmap='gray')\n",
        "    if labels is not None:\n",
        "      axes[i].set_title(CLASS_LABELS[labels[i]], fontsize=28)\n",
        "    plt.setp(axes[i].get_xticklabels(), visible=False)\n",
        "    plt.setp(axes[i].get_yticklabels(), visible=False)\n",
        "  ff.subplots_adjust(wspace=0.1)\n",
        "  plt.show()\n",
        "\n",
        "def load_dataset(split: str, batch_size: int) -> Generator[Batch, None, None]:\n",
        "  ds = tfds.load(\"fashion_mnist\", split=split, shuffle_files=True)\n",
        "  def normalize(inp):\n",
        "    inp['image'] = tensorflow.image.convert_image_dtype(\n",
        "        inp['image'], tensorflow.float32)\n",
        "    return inp\n",
        "  ds = ds.map(normalize)\n",
        "  ds = ds.repeat(2)  # To shuffle across epoch boundary.\n",
        "  SHUFFLE_LENGTH = 50\n",
        "  ds = ds.shuffle(buffer_size=SHUFFLE_LENGTH * batch_size)\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(buffer_size=5)\n",
        "  ds = ds.repeat()\n",
        "  return tfds.as_numpy(ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g2gv_KUIe7i",
        "colab_type": "text"
      },
      "source": [
        "We will be using the Fashion MNIST dataset; let's download it and inspect it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW6QMDgKl1l7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Load and visualize the dataset we will be working with\n",
        "\n",
        "train = load_dataset(\"train\", batch_size=128);\n",
        "batch = next(train)\n",
        "for i in range(0, 20, 10):\n",
        "  gallery(batch['image'][i:i+10], batch['label'][i:i+10])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycE4c1l32w3Z",
        "colab_type": "text"
      },
      "source": [
        "Let's get to know our data a bit more. Find out the following information (search it up or inspect data):\n",
        "\n",
        "* the classes,\n",
        "* shape of data,\n",
        "* representation of each pixel.\n",
        "\n",
        "Do note that we applied some processing on the data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUWw0dje6plC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feel free to play with data here.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDz6RvDAp_rU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Run checks\n",
        "assert batch['image'].shape == (128, 28, 28, 1)\n",
        "print(\"Congratulations! You have completed Task 1.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsgEyOo4mxm4",
        "colab_type": "text"
      },
      "source": [
        "# Task 2: Implement and train an AutoEncoder (AE)\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.dropbox.com/s/zmvci77ih2u350k/ae.png?dl=1\">\n",
        "</center>\n",
        "\n",
        "AutoEncoders map (*encode*) their input into a new representation, then map (*decode*) that new representation back into the input space, with the goal of reconstructing the input.\n",
        "\n",
        "If the input is denoted by $x$, the encoder $E$ and the decoder $D$, the reconstruction is $\\hat{x} = D(E(x))$. In order to encourage reconstruction, we will minimize the mean squared error\n",
        "\n",
        "<font size=4>\n",
        "$$ \\mathcal{L}(x, \\hat{x}) = \\big|\\big|x - D \\big(E(x)\\big)\\big|\\big|^2.$$\n",
        "</font>\n",
        "\n",
        "<br />\n",
        "\n",
        "The space of representations is often called the latent space. We are interested in AEs as this latent space can potentially be a smaller dimensional and better representation of our data. We may also generate new data examples with an autoencoder, but let's return to this later.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McDGTiWtI_uB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We will first implement the autoencoder model, then the training mechanism. Much of the code is given, however you need to fill in some of its key components.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9O8YsqN-fdA",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Modeling\n",
        "\n",
        "For an encoder with a single linear hidden layer with ReLU before mapping to the latent, ie $\\mathrm{Encoder}(x) = z$ where:\n",
        "\n",
        "$$\n",
        "h_0 := \\mathrm{ReLU}(W_0 x + b_0)  \\\\\n",
        "z := W_1 h_0 + b_1  \\\\\n",
        "$$\n",
        "\n",
        "We will use two such hidden linear layers for more capacity.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04Y8WE1nnvYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Encoder\n",
        "class Encoder(hk.Module):\n",
        "  \"\"\"Encoder with linear layers and ReLU.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      hidden_sizes: Sequence[int] = (512, 512),\n",
        "      latent_size: int = 10\n",
        "    ):\n",
        "    super().__init__()\n",
        "    self._hidden_sizes = hidden_sizes\n",
        "    self._latent_size = latent_size\n",
        "\n",
        "  def __call__(self, x: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "    x = hk.Flatten()(x) # (batch_size, w, d) -> (batch_size, w*d)\n",
        "    # ADD CODE BELOW to calculate the returned z latent. --------\n",
        "\n",
        "\n",
        "    # Your code ABOVE. ------------------------------------------\n",
        "    return z\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOo_dxVMBoh2",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Run checks on Encoder implementation\n",
        "# Requires a bit of boilerplate.\n",
        "rng_seq = hk.PRNGSequence(42)\n",
        "model = hk.transform(lambda x: Encoder(latent_size=2)(x), apply_rng=False)  \n",
        "params = model.init(next(rng_seq), jnp.zeros((1, *MNIST_IMAGE_SHAPE)))\n",
        "assert model.apply(params, jnp.zeros(shape=(128, 28, 28, 1))).shape == (128, 2)\n",
        "print(\"Congratulations! Encoder implementation looks good.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8fFilLR-4Bk",
        "colab_type": "text"
      },
      "source": [
        "Our decoder is similar. For a decoder that's a sequence of two linear maps with a ReLU we have $\\mathrm{Decoder}(z) = \\hat{x}$ where:\n",
        "\n",
        "$$\n",
        "h_0' := \\mathrm{ReLU}(W_0' z + b_0')  \\\\\n",
        "\\hat{x} := W_1' h_0' + b_1'  \\\\\n",
        "$$\n",
        "\n",
        "We will use two hidden linear layers for more capacity.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wq0bx358u2W",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Decoder\n",
        "\n",
        "class Decoder(hk.Module):\n",
        "  \"\"\"Decoder model.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      hidden_sizes: Sequence[int] = (512, 512),\n",
        "      output_shape: Sequence[int] = MNIST_IMAGE_SHAPE,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self._hidden_sizes = hidden_sizes\n",
        "    self._output_shape = output_shape\n",
        "\n",
        "  def __call__(self, z: jnp.ndarray) -> jnp.ndarray:\n",
        "    flat_data_dimension = jnp.prod(self._output_shape)\n",
        "    h = z\n",
        "    # ADD CODE BELOW to map back to the data space. --------\n",
        "    # Note, the final reshape is done for you below, you need to calculate\n",
        "    # `flat_x_reconstructed` which is a batch of flat_data_dimension-long\n",
        "    # vectors.\n",
        "\n",
        "\n",
        "    # Your code ABOVE. ------------------------------------------\n",
        "\n",
        "    x_reconstructed = jnp.reshape(\n",
        "        flat_x_reconstructed, (-1, *self._output_shape))\n",
        "\n",
        "    return x_reconstructed\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBMBXm9OU40H",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Run checks on Decoder implementation\n",
        "# (requires a bit of boilerplate)\n",
        "rng_seq = hk.PRNGSequence(42)\n",
        "model = hk.transform(lambda x: Decoder(hidden_sizes=(512, 256), output_shape=(14,14))(x), apply_rng=False)  \n",
        "params = model.init(next(rng_seq), jnp.zeros((1, 2)))\n",
        "assert model.apply(params, jnp.zeros(shape=(128, 2))).shape == (128, 14, 14)\n",
        "print(\"Congratulations! Decoder implementation looks good.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p9Kkc37B_mo",
        "colab_type": "text"
      },
      "source": [
        "Finally we combine the encoder and decoder into an AutoEncoder!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hAvyJdD8wMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title AutoEncoder\n",
        "\n",
        "class AutoEncoder(hk.Module):\n",
        "  \"\"\"Main AE model class, uses Encoder & Decoder under the hood.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      hidden_sizes: Sequence[int] = (512, 512),\n",
        "      latent_size: int = 10,\n",
        "      output_shape: Sequence[int] = MNIST_IMAGE_SHAPE,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self._hidden_sizes = hidden_sizes\n",
        "    self._latent_size = latent_size\n",
        "    self._output_shape = output_shape\n",
        "    self._encoder = Encoder(self._hidden_sizes, self._latent_size)\n",
        "    self._decoder = Decoder(self._hidden_sizes, self._output_shape)\n",
        "\n",
        "  def reconstruct(self, x: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "    z = self._encoder(x)\n",
        "    x_rec = self._decoder(z)\n",
        "    return x_rec, z\n",
        "\n",
        "  def sample(self, z: jnp.ndarray) -> Tuple[jnp.ndarray]:\n",
        "    return self._decoder(z)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRZAaiv5-aXo",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "\n",
        "To complete the training script we need code for:\n",
        "\n",
        "1. the loss function,\n",
        "2. constructing a model and updating its parameters based on data,\n",
        "3. fetching the data and making the parameter updates.\n",
        "\n",
        "How would you\n",
        "* qualitatively evaluate these reconstructions?\n",
        "* quantitatively evaluate your model?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_XJ_MmvOHhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mse_loss(model_outputs: jnp.ndarray, true_images) -> jnp.ndarray:\n",
        "  \"\"\"Mean Squared Error loss function.\"\"\"\n",
        "  # ADD CODE BELOW to calculate the MSE between the true data and --------\n",
        "  # the reconstruction.\n",
        "\n",
        "\n",
        "  # Your code ABOVE. ------------------------------------------\n",
        "  return mse\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAXk0blXRJ-p",
        "colab_type": "text"
      },
      "source": [
        "We create a model and an optimizer, then an update function that works with these. While the code is already written for you, do check it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELh0mEOUPTd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AE_LATENT_SIZE = 2  # This is so we can visualize it easily.\n",
        "ae_model = hk.transform(\n",
        "    lambda x: AutoEncoder(latent_size=AE_LATENT_SIZE).reconstruct(x), apply_rng=True)  \n",
        "ae_optimizer = optix.adam(0.0001)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def mse_of_reconstruction(\n",
        "    params: hk.Params, rng_key: PRNGKey, data_batch) -> jnp.ndarray:\n",
        "  outputs, _ = ae_model.apply(params, rng_key, data_batch[\"image\"])\n",
        "  return mse_loss(outputs, data_batch['image'])\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def update(\n",
        "    params: hk.Params, rng_key: PRNGKey, opt_state: OptState, batch: Batch\n",
        ") -> Tuple[hk.Params, OptState]:\n",
        "  \"\"\"Single update step.\"\"\"\n",
        "  grads = jax.grad(mse_of_reconstruction)(params, rng_key, batch)\n",
        "  loss = mse_of_reconstruction(params, rng_key, batch)\n",
        "  updates, new_opt_state = ae_optimizer.update(grads, opt_state)\n",
        "  new_params = optix.apply_updates(params, updates)\n",
        "  return new_params, new_opt_state, loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY3dFVANSO0d",
        "colab_type": "text"
      },
      "source": [
        "Finally we hook everything up. During the course of training we will show reconstructions on the validation data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SzWIAwFdg53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RANDOM_SEED = 13\n",
        "rng_seq = hk.PRNGSequence(RANDOM_SEED)\n",
        "ae_params = ae_model.init(next(rng_seq), jnp.zeros((1, *MNIST_IMAGE_SHAPE)))\n",
        "opt_state = ae_optimizer.init(ae_params)\n",
        "\n",
        "train_ds = load_dataset(tfds.Split.TRAIN, 128)\n",
        "valid_ds = load_dataset(tfds.Split.TEST, 128)\n",
        "# IMPORTANT! Normally you would use a different validation and test set.\n",
        "\n",
        "training_stats = defaultdict(list)\n",
        "for step in range(1000):\n",
        "  ae_params, opt_state, loss = update(ae_params, next(rng_seq), opt_state, next(train_ds))\n",
        "  \n",
        "  if step % 100 == 0:\n",
        "    valid_batch = next(valid_ds)\n",
        "    rng_key = next(rng_seq)\n",
        "    val_loss = mse_of_reconstruction(ae_params, rng_key, valid_batch)\n",
        "    print(f\"STEP: {step}; Validation loss: {val_loss}\")\n",
        "    \n",
        "    training_stats['val_loss'].append(val_loss)\n",
        "    training_stats['train_loss'].append(loss)\n",
        "    training_stats['step'].append(step)\n",
        "\n",
        "    valid_rec, _ = ae_model.apply(ae_params, rng_key, valid_batch[\"image\"])\n",
        "    gallery(valid_rec, valid_batch['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_RA3DbQShCF",
        "colab_type": "text"
      },
      "source": [
        "We show originals and reconstructions on the training and validation sets. Doing this on the training set is already done for you, you need to do it for the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJVBSRv5SsDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch = next(train_ds)\n",
        "valid_batch = next(valid_ds)\n",
        "\n",
        "print(\"Training set original then reconstruction under it:\")\n",
        "train_rec, _ = ae_model.apply(ae_params, rng_key, train_batch[\"image\"])\n",
        "gallery(train_batch['image'], train_batch['label'])\n",
        "gallery(train_rec)\n",
        "\n",
        "print(\"Validation set original then reconstruction under it:\")\n",
        "# ADD CODE BELOW to get the sample(s). --------\n",
        "\n",
        "\n",
        "# Your code ABOVE. ------------------------------------------\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI9eR-rJgfQT",
        "colab_type": "text"
      },
      "source": [
        "Let's check the training and validation losses. Run the checks in the cell after to verify that it is roughly on the right scale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htBQP5_GfweG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(training_stats['step'], training_stats['train_loss'], label=\"Training loss\")\n",
        "plt.plot(training_stats['step'], training_stats['val_loss'], label=\"Validation loss\")\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMY9INDHinzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run checks\n",
        "assert min(training_stats['train_loss']) < 0.050\n",
        "print(\"Congratulations! Your trained your AutoEncoder.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giapi-1dd111",
        "colab_type": "text"
      },
      "source": [
        "**Questions:**\n",
        "\n",
        "* What can you say given the training and validation losses you see? \n",
        "* Do you think overall this works well?\n",
        "* Do you see some systematic problems?\n",
        "* Why do you think bags are poorly reconstructed?\n",
        "\n",
        "Finally, hypothetically speaking, what mechanism would you us to create new images? Give this a little thought before continuing, we'll circle back to it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxJPgTVdiD_p",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Task 3: Playing with the latent space of our AE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfTD5w-BJgEG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Visualization of the latent space\n",
        "\n",
        "We trained an autoencoder with a 2D latent space, making it easy to visualize what latents --what representations-- data gets mapped to.\n",
        "\n",
        "We will plot $\\mathrm{E}(x)$ for 10 batches of validation data, colouring them by their class.\n",
        "\n",
        "Before looking at the visualization, let's think about what you may expect it to look like. Here are some questions, but feel free to think up more:\n",
        "* Do you expect data from the same classes to be close to each other or not?\n",
        "* Can you think of some choices in our model and/or training that has an effect on the latent space?\n",
        "* What parts of the latent space will be covered? Will there be \"holes\" in the latent space?\n",
        "\n",
        "You could even make a quick sketch in paper of what you may expect to see."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxwDeZeTNVEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h, y = [], []\n",
        "for _ in range(75):\n",
        "  valid_batch = next(valid_ds)\n",
        "  h.append(ae_model.apply(ae_params, next(rng_seq), valid_batch[\"image\"])[1])\n",
        "  y.append(valid_batch['label'])\n",
        "h = np.concatenate(h, axis=0)\n",
        "y = np.concatenate(y, axis=0)\n",
        "plt.figure(figsize=(7,7))\n",
        "# plt.grid()\n",
        "for yy in range(10):\n",
        "  plt.scatter(h[y==yy][:, 0], h[y==yy][:, 1], label=CLASS_LABELS[yy], s=7)\n",
        "\n",
        "plt.scatter([0],[0], marker=\"x\", s=200, color=\"red\", linewidth=4)\n",
        "plt.axis('equal')\n",
        "plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9adH2xu1i8Oz",
        "colab_type": "text"
      },
      "source": [
        "Is this similar to your expectations? Do note that this was just a single run with a single initialization -- does rerunning with a *different random seed* change the image considerably?\n",
        "\n",
        "If you haven't thought about how you would generate a new image yet, this is the time since we will be giving it away in the next section!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAkimsYxOA1_",
        "colab_type": "text"
      },
      "source": [
        "### Generating new data\n",
        "\n",
        "In order to generate some new data, we will simply decode (with $D$) a point in the latent space. The code for setting this up is provided below, you'll have to use it to create the new image.\n",
        "\n",
        "Before running the code try to think about what will happen if you decode from the (0, 0) (red mark in the previous plot)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5d3w7cekpza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How this works with jax and haiku:\n",
        "# \n",
        "# Recall, our autoencoder had a reconstruct and a sample function. Both of\n",
        "# these function shared the same decoder. We already trained the decoder --\n",
        "# as part of the model. The trained parameters are in `ae_params`.\n",
        "#\n",
        "# We create a new ae_sampler model and simply inject these sampling params\n",
        "# from `ae_params` into the model.\n",
        "ae_sampler = hk.transform(\n",
        "    lambda x: AutoEncoder(latent_size=AE_LATENT_SIZE).sample(x), apply_rng=True)  \n",
        "\n",
        "params_for_sampling = ae_sampler.init(\n",
        "    next(rng_seq), jnp.zeros((1, AE_LATENT_SIZE)))\n",
        "\n",
        "# Use the trained parameters from the autoencoder for sampling.\n",
        "ae_params  # has keys like ... 'auto_encoder/~/decoder/linear', 'auto_encoder/~/decoder/linear_1', 'auto_encoder/~/encoder/linear', 'auto_encoder/~/encoder/linear_1'\n",
        "params_for_sampling  # has keys 'auto_encoder/~/decoder/linear', 'auto_encoder/~/decoder/linear_1'\n",
        "\n",
        "trained_params_for_sampling = {}\n",
        "for key in params_for_sampling:\n",
        "  trained_params_for_sampling[key] = ae_params[key]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7nFpoXsmEyg",
        "colab_type": "text"
      },
      "source": [
        "**Exercise.** Decode some points in the latent space and visualize them.  Start with the point (0, 0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFErtDLWnv8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ADD CODE BELOW to get the sample(s). --------\n",
        "# You can use ae_sample as ae_sampler.apply(params, next(rng_seq), z)\n",
        "\n",
        "\n",
        "\n",
        "# Your code ABOVE. ------------------------------------------\n",
        "\n",
        "gallery(sample, max_fig_size=(min(2*sample.shape[0], 20), 2))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7ahbDTgkqDm",
        "colab_type": "text"
      },
      "source": [
        "What did you observe when sampling across the latent space? Did the example decoded from the (0, 0) point match your expectation?\n",
        "\n",
        "\n",
        "**Question:** what could you do to automate the process of picking latents that will decode into nice images?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNfvvYgOnv7o",
        "colab_type": "text"
      },
      "source": [
        "# Task 4: Implement and train a Variational AutoEncoder\n",
        "\n",
        "\n",
        "Variational AutoEncoders (VAEs) also have an encoder, a decoder and try to reproduce data. However VAEs aim to learn the distribution of data and not simply reconstruct individual images.\n",
        "\n",
        "Here both the encoder and the decoder can be stochastic and we get to think about distributions in the latent and data spaces. This story gets a bit more complex than that of a vanilla AutoEncoder, so let us start by introducing the characters:\n",
        "\n",
        "* $P^*$ is the true data distribution. We have some samples from this.\n",
        "* $p(z)$ is a *prior* distribution over the latent space.\n",
        "* $E(x)$ the encoder retains its role, except now it outputs distributions over the latent space $Z$, not just elements of it. The produced distribution is denoted $q_\\phi(z|x)$ and is the (approximate) *posterior* distribution.\n",
        "* $D(z)$ the decoder may be stochastic again, modeling the likelihood distribution $p_\\theta(x|z)$.\n",
        "\n",
        "Now we go over the reconstruction and sampling process and finally motivate the loss used in VAEs.\n",
        "\n",
        "### Reconstruction\n",
        "The process for reconstruction is still simple:\n",
        "\n",
        "1. Take $x_0 \\sim P^*$.\n",
        "2. Encode it $E_\\phi(x_0)$, yielding $q_\\phi(z|x_0)$.\n",
        "3. Sample a latent $z_0 \\sim q_\\phi(z|x_0)$.\n",
        "4. Decode the latent $p_\\theta(x|z_0) = D_\\theta(z_0)$.\n",
        "5. Sample a reconstruction: $x_0' \\sim p_\\theta(x|z_0)$.\n",
        "\n",
        "The prior has not showed up here, it plays a role in sampling.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1auGKK_J6nbQi1nw4fSNcJ2wqjVLzAJk5\" width=\"450\" alt=\"Illustration of VAE reconstruction and sampling\" /></center>\n",
        "\n",
        "### Sampling\n",
        "The sampling process is still simple and reminiscent of our data generation in the AutoEncoder:\n",
        "\n",
        "1. Sample a latent $z_0 \\sim p(z)$ from the prior.\n",
        "2. Decode the latent $p_\\theta(x|z_0) = D_\\theta(z_0)$.\n",
        "3. Sample a reconstruction: $x_0 \\sim p_\\theta(x|z_0)$.\n",
        "\n",
        "In practice we usually use simple, parametrizable distributions in the encoder and decoder. More specifically..\n",
        "\n",
        "**Encoder**\n",
        "Each latent dimension is a (univariate) gaussian, parametrized by the mean and standard deviation. Note, this is the same as a multivariate guassian over the latent space with a diagional covariance matrix.\n",
        "\n",
        "**Decoder**\n",
        "We will quantize the pixels to 0 and 1, which allows us to use a Bernoulli distribution per pixel to model it. Though for visualizations we will continue to use grayscale values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ie6QsDwYfoE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Modeling\n",
        "\n",
        "Let's implement our model!\n",
        "\n",
        "Our Encoder uses a two hidden layer before mapping into latent space. It now outputs a guassian distribution; we can achieve this by outputting (and later learning) the mean and the variance for the gaussian.\n",
        "\n",
        "Mathematically speaking (with one hidden layer), for input $x$:\n",
        "\n",
        "$$\n",
        "h_0 := \\mathrm{ReLU}(W_0 x + b_0)  \\\\\n",
        "\\mu := W_1 h_0 + b_1  \\\\\n",
        "\\log \\sigma := W_2 h_0 + b_2  \\\\\n",
        "$$\n",
        "\n",
        "where $\\mu, \\log \\sigma$ are vectors of length `latent_size`. Then\n",
        "\n",
        "$$q_\\phi(z|x) = \\mathcal{N}(\\mu, \\mathrm{diag}( \\sigma^2) ).$$\n",
        "\n",
        "**Question:** why do we do $\\log \\sigma := W_2 h_0 + b_2$ instead of simply $\\sigma := W_2 h_0 + b_2$? Can you think of other alternatives?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYX7UqhqqY2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAEEncoder(hk.Module):\n",
        "  \"\"\"VAE Encoder model.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      hidden_sizes: Sequence[int] = (512, 512),\n",
        "      latent_size: int = 10):\n",
        "    super().__init__()\n",
        "    self._hidden_sizes = hidden_sizes\n",
        "    self._latent_size = latent_size\n",
        "\n",
        "  def __call__(self, x: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "    h = hk.Flatten()(x)\n",
        "    for hs in self._hidden_sizes:\n",
        "      h = hk.Linear(hs)(h)\n",
        "      h = jax.nn.relu(h)\n",
        "\n",
        "    # ADD CODE BELOW to find mean (\\mu) and standard deviation (\\sigma). ------\n",
        "    # They should be of size self._latent_size.\n",
        "\n",
        "\n",
        "    # Your code ABOVE. --------------------------------------------------------\n",
        "\n",
        "    return mean, stddev\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znOUL4XOdDQe",
        "colab_type": "text"
      },
      "source": [
        "We will reuse the Decoder from the autoencoder, though we will need to tweak its output so it can be used as a parameter for the Bernoulli distribution. We will pass the output through a sigmoid to smoothly bring it into a the [0,1] range.\n",
        "\n",
        "In the cell below there are two sections to fill:\n",
        "1. Combining the encode and decode functions into a \"reconstruction\" function.\n",
        "2. Completing the code for sampling from $\\mathcal{N}(\\mu, \\sigma^2)$,\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh2lQ3QIbq4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RecOutput = namedtuple(\n",
        "    'RecOutput',\n",
        "    ['mean_image', 'sampled_image', 'logits', 'z', 'mean', 'stddev']\n",
        ")\n",
        "\n",
        "\n",
        "class VariationalAutoEncoder(hk.Module):\n",
        "  \"\"\"Main VAE model class, uses Encoder & Decoder under the hood.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      hidden_sizes: Sequence[int] = (512, 512),\n",
        "      latent_size: int = 10,\n",
        "      output_shape: Sequence[int] = MNIST_IMAGE_SHAPE,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self._hidden_sizes = hidden_sizes\n",
        "    self._latent_size = latent_size\n",
        "    self._output_shape = output_shape\n",
        "    self._encoder = VAEEncoder(self._hidden_sizes, self._latent_size)\n",
        "    self._decoder = Decoder(self._hidden_sizes, self._output_shape)\n",
        "\n",
        "  def reconstruct(self, x):\n",
        "    # ADD CODE BELOW combining self.encode() and self.decode() ------\n",
        "\n",
        "\n",
        "\n",
        "    # Your code ABOVE. ------------------------------------------\n",
        "    return RecOutput(mean_image, sampled_image, logits, z, mean, stddev)\n",
        "\n",
        "  def encode(self, x):\n",
        "    \"\"\"Return a single sample from q_\\phi(z|x). Also return mean and stddev.\"\"\"\n",
        "    mean, stddev = self._encoder(x)\n",
        "    # We sample from N(0, 1), and scale and shift the distribution to make\n",
        "    # it distributed according to N(mean, stddev^2).\n",
        "    normal_sample = jax.random.normal(hk.next_rng_key(), mean.shape)\n",
        "\n",
        "    # ADD CODE BELOW to scale and shift the sample into the required `z` ------\n",
        "    # You can use the fact that for X ~ N(mu, sigma^2), real constants a, c:\n",
        "    # cX ~ N(c * mu, c^2 * sigma^2)\n",
        "    # a + X ~ N(a + mu, sigma^2)\n",
        "    # \n",
        "    # Work out how to turn a sample from N(0, 1) into one from\n",
        "    # N(mean, stddev^2).\n",
        "\n",
        "    # Your code ABOVE. ------------------------------------------\n",
        "    return z, (mean, stddev)\n",
        "\n",
        "  def decode(self, z):\n",
        "    \"\"\"Decode z into the bernoulli params and a sample from them.\"\"\"\n",
        "    logits = self._decoder(z)\n",
        "    mean_image = jax.nn.sigmoid(logits)\n",
        "    sampled_image = jax.random.bernoulli(hk.next_rng_key(), mean_image)\n",
        "    return logits, mean_image, sampled_image\n",
        "\n",
        "  def sample(self): \n",
        "    \"\"\"Sample from the prior (see Sampling section at the beginning \n",
        "    of this section).\"\"\"\n",
        "    z = jax.random.normal(hk.next_rng_key(), (1, self._latent_size))  \n",
        "    logits, mean_image, sampled_image = self.decode(z)\n",
        "    return mean_image, sampled_image, z\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw0D_5HbsWSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rng_seq = hk.PRNGSequence(42)\n",
        "test_model = hk.transform(\n",
        "    lambda x: VariationalAutoEncoder(output_shape=(7,7)).reconstruct(x),\n",
        "    apply_rng=True)\n",
        "test_params = test_model.init(next(rng_seq), jnp.zeros((1, 3)))\n",
        "assert test_model.apply(\n",
        "    test_params, next(rng_seq), jnp.zeros(shape=(128, 3)))[0].shape == (128, 7, 7)\n",
        "print(\"Congratulations! Your VAE implementation runs.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbfUm0P_6jhx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## The Loss\n",
        "\n",
        "We want our **sampling** model (=decode latents sampled from the prior) to closely reproduce the true data and our loss needs to capture this.\n",
        "\n",
        "We use maximum likelihood for training, that is, we want to maximize\n",
        "\n",
        "$$\\mathbb{E}_{x \\sim P^*}\\log p_{\\theta}(x).$$\n",
        "<br>\n",
        "\n",
        "Note that $p_{\\theta}(x)$ is the marginal probability distribution $p_{\\theta}(x) = \\int p_\\theta(x, z) dz $. We can rewrite this in familiar terms as $ \\int p_\\theta(x|z) p(z) dz $. However, computing (and maximizing) the above marginal is computationally infeasible.\n",
        "\n",
        "Instead, we can show\n",
        "\n",
        "$$\\log p_{\\theta}(x) \\ge \\mathbb{E}_{z \\sim q(z|x)} \\big[\\log p_\\theta(x | z)\\big] - \\mathbb{KL}\\big(q_\\phi(z | x) || p(z)\\big).$$\n",
        "\n",
        "This right hand side is called the evidence lower bound (ELBO). We do not derive it here, simply use it. Variational methods, variational inference, refers to this technique of using an approximate posterior distribution and the ELBO, and this is where VAE: Variational Autoencoder gets its name from.\n",
        "\n",
        "In order to try to maximize the likelihood, we maximize the ELBO instead. Recall from the lecture that under some conditions (that are not going to apply to us) the inequality is actually an equality. This yield the following loss used with Variational AutoEncoders:\n",
        "\n",
        "<font size=4>\n",
        "<br>\n",
        "<!-- $$ \\mathcal{L}(X, z) = \\mathbb{E}\\big[\\log P(X|z)\\big] - D_{KL}\\big[Q(z|X) \\big|\\big| P(z)\\big].$$ -->\n",
        "\n",
        "$$ \\mathcal{L}(x) = - \\Big( \\mathbb{E}_{z \\sim q(z|x)} \\big[\\log p_\\theta(x | z)\\big] - \\mathbb{KL}\\big(q_\\phi(z | x) || p(z)\\big) \\Big).$$\n",
        "</font>\n",
        "<br>\n",
        "\n",
        "Observe that:\n",
        "* The first term encourages the model to reconstruct the input faithfully. This part is similar to the Vanilla AutoEncoder.\n",
        "* The second term can be seen as a regularization term of the encoder towards the prior.\n",
        "\n",
        "(The formula contains an expectation; in practice that would be approximated with one or more samples.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgRwJ-Kr0aPS",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**The KL for our Gaussian $q_\\phi(z|x)$ and prior**\n",
        "\n",
        "Recall are using gaussians to model $q_\\phi(z|x)$. Furthermore, the prior will be $\\mathcal{N}(0, I)$, the centered unit variance gaussian. This makes the KL possible to compute analytically! Furthermore, we can make the calculation for a univariate guassian since our components are independent. We find:\n",
        "\n",
        "$$ \\mathbb{KL}\\big( \\mathcal{N}(\\mu, \\sigma^2) || \\mathcal{N}(0, 1) \\big) = \\frac12 \\big(\\sigma^2  - \\log(\\sigma^2) + \\mu^2 - 1 \\big).$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2psFvi1XBS1",
        "colab_type": "text"
      },
      "source": [
        "## Evidence Log-likelihood Lower Bound (ELBO) \n",
        "\n",
        "Please fill in missing parts in the loss function implementation that calculate the ELBO."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEN4yk7YLzrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def kl_gaussian(mean: jnp.ndarray, variance: jnp.ndarray) -> jnp.ndarray:\n",
        "  r\"\"\"Calculate KL divergence between given and standard gaussian distributions.\n",
        "  \"\"\"\n",
        "  # ADD CODE BELOW to make the calculation according to the formula above. ----\n",
        "\n",
        "\n",
        "\n",
        "  # Your code ABOVE. ------------------------------------------\n",
        "  return jnp.sum(kl_divergence_vector, axis=-1)  # Sum over latent dimensions.\n",
        "\n",
        "\n",
        "mean_vectors = jnp.array([\n",
        "    [0.25, 0.0],  # Example 1.\n",
        "    [0.5, 0.0],  # Example 2.\n",
        "])\n",
        "variance = jnp.array([\n",
        "    [1, 1],  # Example 1.\n",
        "    [0.1, 1],  # Example 2.\n",
        "])\n",
        "expected = jnp.array([\n",
        "    0.03125,  # Example 1.\n",
        "    0.8262926,  # Example 2.\n",
        "])\n",
        "assert np.allclose(kl_gaussian(mean_vectors, variance), expected)\n",
        "print(\"Congratulations! Your implementation of kl_gaussian looks good!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qVBgxwY9_Sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def elbo(x: jnp.ndarray, logits: jnp.ndarray, \n",
        "         mean: jnp.ndarray, stddev: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Calculate the ELBO.\n",
        "    \n",
        "    Args:\n",
        "      x: data whose likelihood we are interested in.\n",
        "      logits: the logodds (logits) of the decoded distribution from the model.\n",
        "      mean: mean of the q(z|x).\n",
        "      stddev: stddev of the q(z|x).\n",
        " \n",
        "    Returns:\n",
        "      A scalar ELBO, the batch of log_likelihoods and kl.\n",
        "    \"\"\"\n",
        "    log_likelihood = -binary_cross_entropy(x, logits)\n",
        "    kl = kl_gaussian(mean, stddev**2)\n",
        "    \n",
        "    # ADD CODE BELOW to calculate the elbo from the above components. ----\n",
        "\n",
        "\n",
        "    # Your code ABOVE. ---------------------------------------------------\n",
        "\n",
        "    return jnp.mean(elbo), jnp.mean(log_likelihood), jnp.mean(kl)\n",
        "\n",
        "\n",
        "def binary_cross_entropy(x: jnp.ndarray, logits: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Calculate binary (logistic) cross-entropy from distribution logits.\n",
        "\n",
        "  Args:\n",
        "    x: input variable tensor, must be of same shape as logits\n",
        "    logits: log odds of a Bernoulli distribution, i.e. log(p/(1-p))\n",
        "  Returns:\n",
        "    A batch of scalars representing binary CE for the given Bernoulli\n",
        "    distribution.\n",
        "  \"\"\"\n",
        "  if x.shape != logits.shape:\n",
        "    raise ValueError(\"inputs x and logits must be of the same shape\")\n",
        "\n",
        "  x = jnp.reshape(x, (x.shape[0], -1))\n",
        "  logits = jnp.reshape(logits, (logits.shape[0], -1))\n",
        "\n",
        "  return -jnp.sum(x * logits - jnp.logaddexp(0.0, logits), axis=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAUxtT4Jwsz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run checks.\n",
        "\n",
        "# This should have KL term 0 (because q(z|x) is gaussian) and be just log2*28^2.\n",
        "v, _, _ = elbo(jnp.ones(shape=(1, 28, 28)), jnp.ones(shape=(1, 28, 28)), \n",
        "     jnp.array([[0.0, 0.0]]), jnp.array([[1.0, 1.0]]))\n",
        "assert np.allclose(v, -245.59708)\n",
        "print(\"Congratulations! Your implementation of elbo looks good!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBLog8qJyxVL",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "This code is already complete, you only need to run it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2VjG2hfy3YL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Training Loop\n",
        "rng_seq = hk.PRNGSequence(42)\n",
        "vae_model = hk.transform(\n",
        "    lambda x: VariationalAutoEncoder(latent_size=2).reconstruct(x),\n",
        "    apply_rng=True)  \n",
        "optimizer = optix.adam(0.0001)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def elbo_fn(params: hk.Params, rng_key: PRNGKey, batch: Batch) -> jnp.ndarray:\n",
        "    \"\"\"Return ELBO and its components\"\"\"\n",
        "    images = batch[\"image\"]\n",
        "    output = vae_model.apply(params, rng_key, images)\n",
        "    return elbo(images, output.logits, output.mean, output.stddev)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def update(params: hk.Params, rng_key: PRNGKey, opt_state: OptState, \n",
        "           batch: Batch) -> Tuple[hk.Params, OptState]:\n",
        "  \"\"\"Single update step.\"\"\"\n",
        "  def _just_loss(params, rng_key, batch):\n",
        "    # NOTE the negative on the elbo!\n",
        "    return -elbo_fn(params, rng_key, batch)[0]\n",
        "\n",
        "  grads = jax.grad(_just_loss)(params, rng_key, batch)\n",
        "  elbo, ll, kl = elbo_fn(params, rng_key, batch)\n",
        "  updates, new_opt_state = optimizer.update(grads, opt_state)\n",
        "  new_params = optix.apply_updates(params, updates)\n",
        "  return new_params, new_opt_state, elbo, ll, kl\n",
        "\n",
        "\n",
        "vae_params = vae_model.init(next(rng_seq), jnp.zeros((1, *MNIST_IMAGE_SHAPE)))\n",
        "opt_state = optimizer.init(vae_params)\n",
        "\n",
        "train_ds = load_dataset(tfds.Split.TRAIN, 128)\n",
        "valid_ds = load_dataset(tfds.Split.TEST, 128)\n",
        "\n",
        "training_stats = defaultdict(list)\n",
        "for step in range(2500):\n",
        "  batch = next(train_ds)\n",
        "  # Binarize like in https://github.com/google/jax/blob/master/examples/mnist_vae.py\n",
        "  batch['image'] = jax.random.bernoulli(next(rng_seq), batch['image'])\n",
        "  vae_params, opt_state, elb, ll, kl = update(vae_params, next(rng_seq), opt_state, batch)\n",
        "  \n",
        "  if step % 250 == 0:\n",
        "    valid_batch = next(valid_ds)\n",
        "    valid_batch['image'] = jax.random.bernoulli(next(rng_seq), valid_batch['image'])\n",
        "    val_elb, val_ll, val_kl = elbo_fn(vae_params, next(rng_seq), valid_batch)\n",
        "    print(f\"STEP: {step}; Validation elbo (loss): {val_elb}\")\n",
        "    \n",
        "    training_stats['val_elbo'].append(val_elb)\n",
        "    training_stats['train_elbo'].append(elb)\n",
        "    training_stats['val_kl'].append(val_kl)\n",
        "    training_stats['train_kl'].append(kl)\n",
        "    training_stats['val_ll'].append(val_ll)\n",
        "    training_stats['train_ll'].append(ll)\n",
        "    training_stats['step'].append(step)\n",
        "\n",
        "    valid_output = vae_model.apply(\n",
        "        vae_params, next(rng_seq), valid_batch[\"image\"])\n",
        "    gallery(valid_output.mean_image, valid_batch['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeCG8vINzlAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title elbo, loglikelihood, kl\n",
        "f, ax = plt.subplots(3, 1, figsize=(11, 7))\n",
        "for key, axx in zip(['elbo', 'll', 'kl'], ax.reshape(-1,)):\n",
        "  axx.plot(training_stats['step'], training_stats[f'train_{key}'], label=f\"Training {key}\")\n",
        "  axx.plot(training_stats['step'], training_stats[f'val_{key}'], label=f\"Validation {key}\")\n",
        "  axx.set_xlabel(\"Training step\")\n",
        "  axx.set_ylabel(key)\n",
        "  axx.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ8dYql_foKc",
        "colab_type": "text"
      },
      "source": [
        "What do we observe based on the plot? What is a possible explanation of this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91W1-gUTXUfI",
        "colab_type": "text"
      },
      "source": [
        "Let's visualize reconstructions for training and validation data. Try visualiazing the mean image and the sampled image. When visualizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcuGdcsgZ7da",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch_viz = next(train_ds)\n",
        "valid_batch_viz = next(valid_ds)\n",
        "train_original_image = train_batch_viz['image']\n",
        "valid_original_image = valid_batch_viz['image']\n",
        "train_batch_viz['image'] = jax.random.bernoulli(next(rng_seq), train_batch_viz['image'])\n",
        "valid_batch_viz['image'] = jax.random.bernoulli(next(rng_seq), valid_batch_viz['image'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnj12Tn1XVB3",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Reconstructions { run: \"auto\" }\n",
        "visualize = 'mean_image' #@param [\"mean_image\", \"sampled_image\"] \n",
        "\n",
        "print(\"Train original then reconstruction under it:\")\n",
        "output = vae_model.apply(vae_params, rng_key, train_batch_viz[\"image\"])\n",
        "train_img = train_original_image if visualize == 'mean_image' else train_batch_viz['image']\n",
        "gallery(train_img, train_batch_viz['label'])\n",
        "gallery(getattr(output, visualize))\n",
        "\n",
        "print(\"Validation original then reconstruction under it:\")\n",
        "output = vae_model.apply(vae_params, rng_key, valid_batch_viz[\"image\"])\n",
        "valid_img = valid_original_image if visualize == 'mean_image' else valid_batch_viz['image']\n",
        "gallery(valid_img, valid_batch_viz['label'])\n",
        "gallery(getattr(output, visualize))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqlLNCJSzh9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run checks\n",
        "assert max(training_stats['train_elbo']) > -300\n",
        "print(\"Congratulations! You have completed Task 4.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTd7_JQBJz3h",
        "colab_type": "text"
      },
      "source": [
        "# Task 5: Examine the latent variables learned by VAE\n",
        "\n",
        "We inspect the latent space learned by the VAE. Recall,\n",
        "\n",
        "1. When training our aim was to learn the true data distribution. More specifically, when we sample from the $\\mathcal{N}(0, I)$ prior and decode those latents the resulting distribution should match that of the true dataset.\n",
        "\n",
        "2. The KL component of the loss had a regularizer effect; in particular, it tries to keep the posteriors $q_\\phi(z|x)$ close $\\mathcal{N}(0, I)$.\n",
        "\n",
        "Let's see what the latent space actually looks like.\n",
        "\n",
        "We encode examples from the validation set and collect a sample from each posterior distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHUuwrTaHdyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Visualizing q_(z|X)\n",
        "## Note: this code assumes model has 2d latent space\n",
        "zs, ys = [], []\n",
        "for _ in range(75):\n",
        "  valid_batch = next(valid_ds)\n",
        "  valid_batch['image'] = jax.random.bernoulli(next(rng_seq), valid_batch['image'])\n",
        "  zs.append(vae_model.apply(vae_params, next(rng_seq), valid_batch[\"image\"]).z)\n",
        "  ys.append(valid_batch['label'])\n",
        "latents = np.concatenate(zs, axis=0)\n",
        "labels = np.concatenate(ys, axis=0)\n",
        "plt.figure(figsize=(9, 7))\n",
        "for yy in range(10):\n",
        "  plt.scatter(\n",
        "      latents[labels==yy][:, 0], latents[labels==yy][:, 1],\n",
        "      label=CLASS_LABELS[yy], s=6)\n",
        "plt.legend()\n",
        "\n",
        "plt.scatter([0],[0], marker=\"x\", s=150, color=\"black\", linewidth=3)\n",
        "plt.axis('equal');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PfS_jbzv7ls",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Questions**:\n",
        "1. Do you expect samples (ie decoded latents from $\\mathcal{N}(0, I)$) to be \"good\"?\n",
        "2. Do you expect samples to actually be similarly distributed to the real data? Why / Why not?\n",
        "3. Can you think of some things that would affect how similar this distribution looks to the $\\mathcal{N}(0, I)$ prior?  These could be ways we parametrize, train, or even new introduce new parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgTJGunX1dsZ",
        "colab_type": "text"
      },
      "source": [
        "Next we do a visual check for question 2. We show a grid of decoded latents.\n",
        "\n",
        "Using the form, you can select between different ways of picking those latents:\n",
        "* 'uniform': a uniform grid over [-2.5, 2.5] x [-2.5, 2.5],\n",
        "* 'prior': the grid is transformed to reflect the density of the prior.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEgxh4RoBspv",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Visualizing samples  {run: 'auto'}\n",
        "how = 'uniform' #@param [\"uniform\", \"prior\"] \n",
        "\n",
        "# Code borrowed from https://github.com/chaitanya100100/VAE-for-Image-Generation\n",
        "decoder_only = hk.transform(\n",
        "    lambda z: VariationalAutoEncoder(latent_size=2).decode(z), apply_rng=True) \n",
        "\n",
        "# display a 2D manifold of the decoded images\n",
        "n = 15  # figure with 15x15 decoded images\n",
        "digit_size = 28\n",
        "figure = np.zeros((digit_size * n, digit_size * n))\n",
        "\n",
        "linspace = np.linspace(0.05, 0.95, n)\n",
        "if how == 'prior':\n",
        "  grid_x = scipy.stats.norm.ppf(linspace)\n",
        "  grid_y = scipy.stats.norm.ppf(linspace)\n",
        "elif how == 'uniform':\n",
        "  grid_x = 6*linspace -3\n",
        "  grid_y = 6*linspace -3\n",
        "else:\n",
        "  assert 'Unrecognized `how` choice.'\n",
        "\n",
        "for i, yi in enumerate(grid_x):\n",
        "    for j, xi in enumerate(grid_y):\n",
        "        z_sample = np.array([[xi, yi]])\n",
        "        # mean_image, sampled_image\n",
        "        xd_logits, xd_mean_image, _ = decoder_only.apply(\n",
        "            vae_params, next(rng_seq), z_sample)\n",
        "        digit = xd_mean_image[0].reshape(digit_size, digit_size)\n",
        "        figure[i * digit_size: (i + 1) * digit_size,\n",
        "               j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "ax_min, ax_max = linspace[0], linspace[-1]\n",
        "plt.imshow(figure, cmap='Greys_r', extent=[ax_min, ax_max, ax_min, ax_max])\n",
        "grid_x_neat = ['{:0.2f}'.format(x) for x in grid_x]\n",
        "plt.yticks(linspace[::2], grid_x_neat[::2])\n",
        "plt.xticks(linspace[::2], grid_x_neat[::2])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFftRdn9oBll",
        "colab_type": "text"
      },
      "source": [
        "# Regularized AutoEncoder\n",
        "\n",
        "Can vanilla autoencoders generate visually good samples? Note that we are not concerned here with the ability to model the data density, but just being able to generate a nice looking image. \n",
        "\n",
        "The authors of https://arxiv.org/pdf/1903.12436.pdf argue that the answer is yes. They argue that by adding regularization and \"ex-post density estimation\" (Sec.4) -- meaning looking at the distribution of latents produced and fitting a distribution to that -- a vanilla autoencoder facilitates generating high quality samples.\n",
        "\n",
        "We highly encourage you to at least check the paper out as it ties in nicely with this tutorial. We do hope you will also find the homework exercise rewarding!\n",
        "\n",
        "## Homework: Implement and experiment with a Regularized AutoEncoder\n",
        "\n",
        "This is a more open-ended exercise than previous in the notebook. \n",
        "\n",
        "Your tasks are: \n",
        "1. Investigate how samples look if we use latent size of 2. Do the samples look better compared to using latent size of 50? If they do, do you have an explanation for why the samples are better? \n",
        "2. Implement and train Regularized AutoEncoder from https://arxiv.org/pdf/1903.12436.pdf by adding the second term to the loss function from Eq (11) in https://arxiv.org/pdf/1903.12436.pdf. \n",
        "3. Investigate whether the added regularization (1) improves the quality of the generated samples when coupled with ex-post density estimation, (2) reduces mean squared error of reconstructed images on the validation set (you can just use mse_loss_fn to calculate it). Remember to check a few different values of the regularization constant.\n",
        "\n",
        "To help you get started, we provide below code that implements a simple version of ex-post density estimation to sample from our AutoEncoder. The sampling procedure is very similar to that used with Variational AutoEncoder. We will sample a latent variable $z$ from a normal distribution and feed it to the decoder. The main difference is that we will estimate mean and variance of the normal distribution.\n",
        "\n",
        "We use a higher dimensional latent space for the purposes of this exercise to better highlight challenges with generating samples without regularizing the autoencoder.  Note, if you want to visualize high dimensional latent spaces, you might want to use the t-sne dimensionality reduction method (e.g. through `sklearn.manifold.TSNE`).\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxFJ-BNRwJO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train a lager dimensional model.\n",
        "# RAE_LATENT_SIZE = 2\n",
        "RAE_LATENT_SIZE = 50\n",
        "rae_model = hk.transform(\n",
        "    lambda x: AutoEncoder(latent_size=RAE_LATENT_SIZE).reconstruct(x), apply_rng=True)  \n",
        "optimizer = optix.adam(0.0001)\n",
        "\n",
        "@jax.jit\n",
        "def mse_loss_fn(params: hk.Params, rng_key: PRNGKey, batch: Batch) -> jnp.ndarray:\n",
        "  \"\"\"Mean Squared Error loss function.\"\"\"\n",
        "  outputs, _ = rae_model.apply(params, rng_key, batch[\"image\"])\n",
        "  return jnp.mean(jnp.power(batch['image'] - outputs, 2.0))\n",
        "\n",
        "@jax.jit\n",
        "def update(params: hk.Params, rng_key: PRNGKey, opt_state: OptState, \n",
        "           batch: Batch) -> Tuple[hk.Params, OptState]:\n",
        "  \"\"\"Single SGD update step.\"\"\"\n",
        "  grads = jax.grad(mse_loss_fn)(params, rng_key, batch)\n",
        "  loss = mse_loss_fn(params, rng_key, batch)\n",
        "  updates, new_opt_state = optimizer.update(grads, opt_state)\n",
        "  new_params = optix.apply_updates(params, updates)\n",
        "  return new_params, new_opt_state, loss\n",
        "\n",
        "rae_params = rae_model.init(next(rng_seq), jnp.zeros((1, *MNIST_IMAGE_SHAPE)))\n",
        "opt_state = optimizer.init(rae_params)\n",
        "\n",
        "train_ds = load_dataset(tfds.Split.TRAIN, 128)\n",
        "valid_ds = load_dataset(tfds.Split.TEST, 128)\n",
        "\n",
        "training_stats = defaultdict(list)\n",
        "for step in range(1000):\n",
        "  rae_params, opt_state, loss = update(rae_params, next(rng_seq), opt_state, next(train_ds))\n",
        "  \n",
        "  if step % 100 == 0:\n",
        "    valid_batch = next(valid_ds)\n",
        "    val_loss = mse_loss_fn(rae_params, next(rng_seq), valid_batch)\n",
        "    print(f\"STEP: {step}; Validation loss: {val_loss}\")\n",
        "    \n",
        "    training_stats['val_loss'].append(val_loss)\n",
        "    training_stats['train_loss'].append(loss)\n",
        "    training_stats['step'].append(step)\n",
        "\n",
        "    valid_rec, _ = rae_model.apply(rae_params, next(rng_seq), valid_batch[\"image\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mLiroSzY0ZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run simple ex-post density estimation\n",
        "latents = []\n",
        "for _ in range(10):\n",
        "  valid_batch = next(valid_ds)\n",
        "  _, z = rae_model.apply(rae_params, next(rng_seq), valid_batch[\"image\"])\n",
        "  latents.append(z)\n",
        "latents = np.concatenate(latents, axis=0)\n",
        "H_mean, H_std = np.mean(latents, axis=0), np.std(latents, axis=0)\n",
        "H_sampled = np.random.normal(loc=H_mean, scale=H_std, size=(2*128, RAE_LATENT_SIZE)) \n",
        "plt.scatter(H_sampled[:, 0], H_sampled[:, 1], s=10)\n",
        "\n",
        "plt.axis('equal');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ggVa5p0u_mW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create model that doesn't use encoder \n",
        "rae_decoder_only = hk.transform(\n",
        "    lambda z: AutoEncoder(latent_size=RAE_LATENT_SIZE).sample(z), apply_rng=True) \n",
        "# Use it to sample from the model\n",
        "for _ in range(10):\n",
        "  H_sampled = np.random.normal(loc=H_mean, scale=H_std, size=(10, RAE_LATENT_SIZE)) \n",
        "  x_decoded = rae_decoder_only.apply(rae_params, next(rng_seq), H_sampled)\n",
        "  gallery(x_decoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6TxlIog87-Q",
        "colab_type": "text"
      },
      "source": [
        "# Further Reading\n",
        "\n",
        "A number of papers investigate sampling and reconstruction, the structure of latent space both theoretically and through empirical studies. Here is a selection of possible further readings:\n",
        "\n",
        "*   [Fixing a broken elbo](https://arxiv.org/abs/1711.00464). Shows that there are different ways of achieving a tight variational bound, either by learning to reconstruct, but not to sample, or vice versa. They also show the importance of the power of the VAE decoder.\n",
        "*   [Taming VAEs](https://arxiv.org/abs/1810.00597) uses constrained optimization to first ensure that the reconstruction loss is minimized up to a chosen threshold, and only after the KL divergence is minimized.\n",
        "*   [Distribution Matching in Variational Inference]( https://arxiv.org/pdf/1802.06847.pdf) analyzes some of the behaviours that we see in VAEs, and tries to quantify how well VAEs can match the prior latent distribution to the marginal posterior distribution. \n",
        "*   [VQ-VAEs](https://arxiv.org/abs/1906.00446) learn discrete variable autoencoders using a lookup into a continuous table of embeddings. This allows them to achieve high compression rates. When VQ-VAE models are composed, they can achieve impressive results, with photo realistic sample quality. \n",
        "*   [Inference Suboptimality in Variational Autoencoders](https://arxiv.org/pdf/1801.03558.pdf) analyzes the importance of both the approximation and amortization gap in VAEs.\n",
        "*   [Variational Inference with Normalizing Flows](https://arxiv.org/abs/1505.05770) and [Improving Variational Inference with Inverse Autoregressive Flow](https://arxiv.org/abs/1606.04934) look at building more powerful posteriors, which allow minimizing the approximation gap in VAEs. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOXRos4Yq6Nh",
        "colab_type": "text"
      },
      "source": [
        "# Credits\n",
        "\n",
        "Inspired by:\n",
        "* https://github.com/deepmind/dm-haiku/blob/master/examples/vae.py\n",
        "* https://github.com/gmum/ml2019-20/blob/master/lab/13_VAE.ipynb\n",
        "* https://github.com/chaitanya100100/VAE-for-Image-Generation\n",
        "* https://jaan.io/what-is-variational-autoencoder-vae-tutorial/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-ALPD2U9j4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}